{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13be12-8bd4-4642-9382-2d0f5c1e8e83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "111/278 [==========>...................] - ETA: 6s - loss: 2.0000 - accuracy: 0.5304"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "angry_dir = \"angry/\" \n",
    "neutral_dir = \"neutral/\" \n",
    "\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "\n",
    "for filename in os.listdir(angry_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        img = cv2.imread(os.path.join(angry_dir, filename))\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        images.append(img)\n",
    "        labels.append([1, 0])\n",
    "\n",
    "\n",
    "for filename in os.listdir(neutral_dir):\n",
    "    if filename.endswith(\".jpg\"):\n",
    "        img = cv2.imread(os.path.join(neutral_dir, filename))\n",
    "        img = cv2.resize(img, (32, 32))\n",
    "        images.append(img)\n",
    "        labels.append([0, 1]) \n",
    "\n",
    "\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "split_ratio = 0.8  \n",
    "split_index = int(len(images) * split_ratio)\n",
    "\n",
    "train_images, val_images = images[:split_index], images[split_index:]\n",
    "train_labels, val_labels = labels[:split_index], labels[split_index:]\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,  \n",
    "    width_shift_range=0.2,  \n",
    "    height_shift_range=0.2, \n",
    "    shear_range=0.2,  \n",
    "    zoom_range=0.2,  \n",
    "    horizontal_flip=True,  \n",
    "    fill_mode='nearest'  \n",
    ")\n",
    "\n",
    "\n",
    "def lr_decay(epoch):\n",
    "    initial_learning_rate = 0.001\n",
    "    decay_rate = 0.9\n",
    "    decay_step = 1000\n",
    "    return initial_learning_rate * (decay_rate ** (epoch // decay_step))\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lr_decay)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(datagen.flow(train_images, train_labels, batch_size=32),\n",
    "                    steps_per_epoch=len(train_images) / 32, epochs=50,\n",
    "                    validation_data=(val_images, val_labels),\n",
    "                    callbacks=[lr_scheduler])\n",
    "\n",
    "train_acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(epochs, train_acc, 'g', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "cap = cv2.VideoCapture(0)  \n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read() \n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = frame[y:y+h, x:x+w]  \n",
    "        resized_img = cv2.resize(face_img, (32, 32))  \n",
    "        normalized_img = resized_img / 255.0  \n",
    "        reshaped_img = np.reshape(normalized_img, (1, 32, 32, 3))  \n",
    "\n",
    "        \n",
    "        result = model.predict(reshaped_img)\n",
    "        prediction = np.argmax(result)\n",
    "\n",
    "       \n",
    "        if prediction == 0:\n",
    "            cv2.putText(frame, \"Angry\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        elif prediction == 1:\n",
    "            cv2.putText(frame, \"Neutral\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    \n",
    "    cv2.imshow('Face Emotion Detection', frame)\n",
    "\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865c951-144e-40cf-aab5-40eb1258e3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
